# PHASE 10.94 IMPLEMENTATION GUIDE
# WORLD-CLASS FULL-TEXT EXTRACTION ENHANCEMENT

**Created:** November 19, 2025
**Phase:** 10.94 - Full-Text Extraction System Overhaul
**Duration:** 12 days (96-120 hours total)
**Priority:** üî• CRITICAL - Current system "does not work fine"
**Type:** System Enhancement - Intelligent Multi-Tier Extraction Architecture
**Dependencies:** Phase 10.93 Complete (Service layer architecture), All 8 free sources integrated
**Reference:** [8_FREE_SOURCES_FULLTEXT_ANALYSIS.md](./8_FREE_SOURCES_FULLTEXT_ANALYSIS.md)
**Pattern:** Phase 10.6 Day 3.5 Service Extraction + Phase 10.91 Testing Standards

---

## üéØ EXECUTIVE SUMMARY

**Problem:** Current full-text extraction system doesn't work fine across 8 different free academic sources, each with different output formats (PDF URLs, direct content, landing pages, etc.).

**Solution:** World-class, innovative 5-tier cascading extraction strategy with intelligent routing, GROBID integration, and source-specific optimization.

**Impact:**
- 6-10x more content extracted (5000+ words vs 500-800 words)
- 95%+ success rate (vs current ~60%)
- Works reliably across all 8 free sources
- Handles PDF, HTML, and API content seamlessly
- Enterprise-grade error recovery and fallbacks

---

## üìä CURRENT STATE ANALYSIS

### 8 Free Academic Sources

| # | Source | Type | What They Provide | Current Issues |
|---|--------|------|-------------------|---------------|
| 1 | **Semantic Scholar** | Aggregator | PDF URL (openAccessPdf) | Works sometimes |
| 2 | **CrossRef** | Metadata | Landing page only | ‚ùå No full-text extraction |
| 3 | **PubMed** | Biomedical | Abstracts + PMID | ‚ùå Missing PMC linkage |
| 4 | **arXiv** | Preprints | Direct PDF URLs | ‚úÖ Works well |
| 5 | **PMC** | Full-text | Direct full-text content | ‚ö†Ô∏è Underutilized |
| 6 | **ERIC** | Education | Conditional PDF URLs | Works sometimes |
| 7 | **CORE** | Open Access | Download URLs | Works sometimes |
| 8 | **Springer** | Publisher | OA PDF URLs | Works for OA only |

### Current Extraction Flow (PROBLEMATIC)

```
Current Waterfall (4 Tiers):
Tier 1: Database cache ‚Üí Often empty (first time)
Tier 2: PMC API + HTML scraping ‚Üí Fails for non-PMC papers
Tier 3: Unpaywall PDF ‚Üí Works but gets low quality (781 words from 5000-word article)
Tier 4: Direct publisher PDF ‚Üí Rarely used

Issues:
- No source-specific routing
- PDF extraction quality poor (pdf-parse limitations)
- HTML extraction limited (few publisher selectors)
- No identifier enrichment (PMID ‚Üí PMC, DOI ‚Üí Unpaywall)
- No GROBID or advanced extraction
- Missing Unpaywall landing page strategy
```

### Problems Identified

1. **PDF Quality**: pdf-parse extracts 781 words from 5000-word article (15% of content)
2. **No Source Intelligence**: Treats all sources the same (should route based on what they provide)
3. **Missing PMC Optimization**: PMC returns full-text directly but we don't use it well
4. **No Identifier Enrichment**: Don't cross-reference PMID ‚Üí PMC, DOI ‚Üí Unpaywall
5. **Limited HTML Extraction**: Only 7 publisher selectors, many fail with 403
6. **No Unpaywall Landing Page**: Have the URL but only use for PDF construction

---

## üöÄ PROPOSED SOLUTION: 5-TIER INTELLIGENT EXTRACTION

### Architecture Overview

```
NEW 5-TIER CASCADING STRATEGY:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TIER 1: Direct Content (Fastest)                            ‚îÇ
‚îÇ - PMC fullText field (if source = PMC)                      ‚îÇ
‚îÇ - ERIC full-text content (if available)                     ‚îÇ
‚îÇ - Success Rate: 100% | Time: 0ms | Quality: EXCELLENT      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì (if failed)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TIER 2: Direct PDF URLs (Fast)                              ‚îÇ
‚îÇ - arXiv PDFs (100% reliable)                                ‚îÇ
‚îÇ - CORE download URLs                                         ‚îÇ
‚îÇ - Semantic Scholar openAccessPdf                             ‚îÇ
‚îÇ - Springer OA PDFs                                           ‚îÇ
‚îÇ - Success Rate: 80% | Time: 1-3s | Quality: GOOD           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì (if failed)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TIER 3: Identifier-Based Lookup (Medium)                    ‚îÇ
‚îÇ - PMID ‚Üí PMC full-text API                                  ‚îÇ
‚îÇ - DOI ‚Üí Unpaywall (PDF + Landing Page)                      ‚îÇ
‚îÇ - DOI ‚Üí CrossRef ‚Üí Publisher metadata                       ‚îÇ
‚îÇ - Success Rate: 60% | Time: 2-5s | Quality: EXCELLENT      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì (if failed)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TIER 4: Advanced Extraction (Slow but Comprehensive)        ‚îÇ
‚îÇ - GROBID structured extraction (PDF ‚Üí XML)                  ‚îÇ
‚îÇ - Publisher HTML with Unpaywall landing page                ‚îÇ
‚îÇ - AI-powered content extraction (LLM-based)                 ‚îÇ
‚îÇ - Success Rate: 40% | Time: 5-15s | Quality: EXCELLENT     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì (if all failed)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TIER 5: Fallback (Last Resort)                              ‚îÇ
‚îÇ - Abstract only (if >150 words)                             ‚îÇ
‚îÇ - Mark as "no full-text available"                          ‚îÇ
‚îÇ - Success Rate: 100% | Time: 0ms | Quality: LIMITED        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã PHASE 10.94 IMPLEMENTATION PLAN

### Duration: 12 Days (96-120 hours)

**Breakdown:**
- **Days 1-2:** Identifier Enrichment Service (16 hours)
- **Day 3:** Source-Specific Routing Logic (8 hours)
- **Days 4-5:** GROBID Integration (Self-Hosted or HuggingFace) (16 hours)
- **Day 6:** Publisher HTML Enhancement (Unpaywall Landing Pages) (8 hours)
- **Days 7-8:** Unified Extraction Orchestrator Service (16 hours)
- **Days 9-10:** Comprehensive Testing (All 8 Sources) (16 hours)
- **Day 11:** Performance Optimization & Caching (8 hours)
- **Day 12:** Documentation & Production Readiness (8 hours)

---

## DAY 1-2: IDENTIFIER ENRICHMENT SERVICE (16 hours)

### üéØ Goal

Create service that enriches papers with missing identifiers by cross-referencing sources.

### Why This Matters

**Example:**
```
User searches PubMed ‚Üí Gets paper with PMID = 12345678
Current: Only has PMID, can't use PMC or Unpaywall
Enhanced: Service finds PMC ID = PMC9876543, DOI = 10.1234/example
Result: Can now use Tier 3 (PMC full-text API) AND Unpaywall
```

### Implementation

**File:** `backend/src/modules/literature/services/identifier-enrichment.service.ts`

**Class Structure:**
```typescript
export class IdentifierEnrichmentService {
  // PMID ‚Üí PMC ID lookup
  async enrichWithPMCId(pmid: string): Promise<string | null>

  // DOI ‚Üí PMID lookup (via PubMed API)
  async enrichWithPMID(doi: string): Promise<string | null>

  // Title/Authors ‚Üí DOI (via CrossRef)
  async enrichWithDOI(title: string, authors?: string[]): Promise<string | null>

  // Semantic Scholar ‚Üí Extract all external IDs
  async enrichFromSemanticScholar(title: string): Promise<ExternalIds>

  // Main orchestrator
  async enrichPaper(paper: Paper): Promise<EnrichedPaper>
}
```

**Key Methods:**

1. **PMID ‚Üí PMC ID** (Most Important)
   - Use NCBI E-utilities elink API
   - Same API we use in PMC service (lines 232-266)
   - Success rate: ~40% (not all PubMed papers in PMC)

2. **DOI ‚Üí PMID**
   - Use PubMed esearch API with DOI filter
   - Enables PMC lookup for non-PubMed papers

3. **Title ‚Üí DOI**
   - Use CrossRef API with title search
   - Fallback for papers without DOI
   - Fuzzy matching to handle slight title variations

4. **Semantic Scholar Enrichment**
   - Query by title
   - Extract all externalIds (DOI, PMID, PMC, arXiv)
   - Most comprehensive source for identifier mapping

### Testing

**Test Coverage:** 80%+

1. Unit tests for each enrichment method
2. Integration test: Paper with only title ‚Üí Full enrichment
3. Integration test: Paper with PMID ‚Üí PMC ID + DOI
4. Integration test: Paper with DOI ‚Üí PMID + PMC ID
5. Error handling: API failures, rate limits, no matches

### Success Metrics

- 70%+ of papers get at least one new identifier
- PMID ‚Üí PMC conversion rate: 40%+
- DOI ‚Üí PMID conversion rate: 70%+
- Title ‚Üí DOI conversion rate: 80%+

---

## DAY 3: SOURCE-SPECIFIC ROUTING LOGIC (8 hours)

### üéØ Goal

Intelligent routing based on what each source provides (don't waste time on methods that won't work).

### Why This Matters

**Example:**
```
BAD (Current):
Paper from arXiv ‚Üí Try PMC API (fails, waste 3s) ‚Üí Try Unpaywall (fails, waste 3s) ‚Üí Finally use arXiv PDF

GOOD (Enhanced):
Paper from arXiv ‚Üí Immediately use arXiv PDF (0.5s) ‚Üí SUCCESS
```

### Implementation

**File:** `backend/src/modules/literature/services/source-aware-extraction-router.service.ts`

**Routing Matrix:**

| Source | Best Strategy | Fallback 1 | Fallback 2 |
|--------|--------------|------------|------------|
| Semantic Scholar | openAccessPdf URL | DOI ‚Üí Unpaywall | PMC fallback if has PMC ID |
| CrossRef | DOI ‚Üí Unpaywall | DOI ‚Üí Publisher HTML | Abstract only |
| PubMed | PMID ‚Üí PMC | DOI ‚Üí Unpaywall | Abstract only |
| arXiv | Direct PDF (100%) | - | - |
| PMC | Direct content (100%) | Constructed PDF URL | - |
| ERIC | Conditional PDF URL | Landing page HTML | Abstract only |
| CORE | Download URL | DOI ‚Üí Unpaywall | Abstract only |
| Springer | OA PDF URL | DOI ‚Üí Unpaywall (if not OA) | Abstract only |

**Logic:**

```typescript
class SourceAwareExtractionRouter {
  async route(paper: Paper): Promise<ExtractionPlan> {
    // Priority 1: Direct content (PMC)
    if (paper.source === 'pmc' && paper.fullText) {
      return { tier: 1, method: 'direct_content', url: null };
    }

    // Priority 2: Guaranteed PDFs (arXiv)
    if (paper.source === 'arxiv' && paper.pdfUrl) {
      return { tier: 2, method: 'direct_pdf', url: paper.pdfUrl };
    }

    // Priority 3: Identifier-based (has PMID or DOI)
    if (paper.pmid) {
      const pmcId = await this.enrichment.getPMCId(paper.pmid);
      if (pmcId) {
        return { tier: 3, method: 'pmc_api', pmcId };
      }
    }

    if (paper.doi) {
      const unpaywallData = await this.unpaywall.query(paper.doi);
      if (unpaywallData.best_oa_location) {
        return { tier: 3, method: 'unpaywall', data: unpaywallData };
      }
    }

    // Priority 4: Advanced extraction
    if (paper.pdfUrl) {
      return { tier: 4, method: 'grobid', url: paper.pdfUrl };
    }

    // Priority 5: Fallback
    return { tier: 5, method: 'abstract_only', abstract: paper.abstract };
  }
}
```

### Testing

1. Test routing for each of 8 sources
2. Test fallback cascade (Tier 1 fails ‚Üí Tier 2 ‚Üí Tier 3 ‚Üí ...)
3. Test performance (routing should be < 100ms)

---

## DAY 4-5: GROBID INTEGRATION (16 hours)

### üéØ Goal

Integrate GROBID for structured PDF extraction (10x better than pdf-parse).

### Why This Matters

**Comparison:**

| Method | Words Extracted | Quality | Time | Success Rate |
|--------|----------------|---------|------|--------------|
| pdf-parse (current) | 781 | Poor (formatting artifacts) | 2s | 70% |
| GROBID | 5000+ | Excellent (structured XML) | 5s | 90% |

**GROBID Benefits:**
- Extracts structured sections (Introduction, Methods, Results, Discussion)
- Preserves citations and references
- Better handling of multi-column layouts
- Handles mathematical equations and formulas
- Returns TEI XML with semantic markup

### Implementation Options

**Option A: Self-Hosted Docker (Recommended for Production)**
```bash
# Start GROBID service
docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.8.0

# Health check
curl http://localhost:8070/api/isalive
```

**Option B: HuggingFace Demo (For Testing)**
```
https://kermitt2-grobid.hf.space/
Limitations: CPU-only, rate limits, quota restrictions
```

**Service Implementation:**

**File:** `backend/src/modules/literature/services/grobid-extraction.service.ts`

```typescript
export class GrobidExtractionService {
  private readonly GROBID_URL = process.env.GROBID_URL || 'http://localhost:8070';

  /**
   * Process PDF with GROBID full-text extraction
   * Returns structured XML with sections
   */
  async processPDF(pdfBuffer: Buffer): Promise<GrobidResult> {
    const formData = new FormData();
    formData.append('input', pdfBuffer, 'paper.pdf');

    const response = await axios.post(
      `${this.GROBID_URL}/api/processFulltextDocument`,
      formData,
      {
        headers: formData.getHeaders(),
        timeout: 30000, // 30s for large PDFs
      }
    );

    const teiXml = response.data;
    return this.parseTEIXml(teiXml);
  }

  /**
   * Parse GROBID TEI XML to structured content
   */
  private parseTEIXml(xml: string): GrobidResult {
    // Extract structured sections:
    // - Title, Abstract
    // - Introduction, Methods, Results, Discussion, Conclusion
    // - References (with DOI links)
    // - Figures and tables metadata

    return {
      title: extractedTitle,
      abstract: extractedAbstract,
      sections: [
        { heading: 'Introduction', content: intro, wordCount: 500 },
        { heading: 'Methods', content: methods, wordCount: 1000 },
        // ...
      ],
      references: extractedReferences,
      totalWords: 5432,
      extractionMethod: 'grobid',
    };
  }
}
```

**Integration with Extraction Flow:**

Add as Tier 4 (between Unpaywall PDF and Fallback):

```typescript
// Tier 4: GROBID structured extraction
if (pdfUrl && !fullText) {
  const pdfBuffer = await this.downloadPDF(pdfUrl);
  const grobidResult = await this.grobid.processPDF(pdfBuffer);

  if (grobidResult.totalWords > 1000) {
    fullText = this.formatGrobidSections(grobidResult.sections);
    fullTextSource = 'grobid';
    fullTextWordCount = grobidResult.totalWords;
  }
}
```

### Testing

1. Test with arXiv PDFs (clean, well-formatted)
2. Test with publisher PDFs (multi-column, complex)
3. Test with scanned PDFs (OCR needed - GROBID limitation)
4. Benchmark extraction time (should be < 10s for 20-page paper)
5. Compare quality: GROBID vs pdf-parse side-by-side

### Docker Setup Guide

**Development:**
```bash
# Add to docker-compose.yml
services:
  grobid:
    image: lfoppiano/grobid:0.8.0
    ports:
      - "8070:8070"
    environment:
      - JAVA_OPTS=-Xmx4g
```

**Production:**
```bash
# Deploy to same server as backend
# Configure nginx reverse proxy
# Add health checks and monitoring
```

---

## DAY 6: PUBLISHER HTML ENHANCEMENT (8 hours)

### üéØ Goal

Extract from Unpaywall landing pages BEFORE falling back to PDF.

### Why This Matters

**User's Example:**
```
ScienceDirect Landing Page: https://www.sciencedirect.com/science/article/pii/S0584854722002142
- Full article: 5000+ words
- Structured HTML: clean sections
- Fast: 1-2s

Current PDF Extraction:
- PDF download: 3-5s
- pdf-parse extraction: 2s
- Result: 781 words (incomplete)
- Total: 5-7s

Enhanced HTML Extraction:
- HTML fetch: 0.5s
- Parse with JSDOM: 0.5s
- Result: 5000+ words (complete)
- Total: 1s
```

### Implementation

**Enhancement to Existing Service:**

**File:** `backend/src/modules/literature/services/html-full-text.service.ts`

**Current Code (Lines 344-412):**
```typescript
// Already has scrapeHtmlFromUrl method
// Already has publisher-specific selectors for:
// - ScienceDirect
// - Springer/Nature
// - MDPI
// - PLOS, Frontiers, JAMA
```

**NEW: Integration with Unpaywall Landing Page:**

**File:** `backend/src/modules/literature/services/pdf-parsing.service.ts`

Add new Tier 2.5 (between current Tier 2 and Tier 3):

```typescript
// NEW TIER 2.5: Publisher HTML from Unpaywall landing page
if (!fullText && paper.doi) {
  this.logger.log(`üîç Tier 2.5: Attempting HTML from Unpaywall landing page...`);

  // Query Unpaywall for landing page URL
  const unpaywallData = await this.queryUnpaywall(paper.doi);

  if (unpaywallData?.best_oa_location?.url_for_landing_page) {
    const landingPageUrl = unpaywallData.best_oa_location.url_for_landing_page;

    // Try HTML extraction with existing service
    const htmlResult = await this.htmlService.scrapeHtmlFromUrl(landingPageUrl);

    if (htmlResult.success && htmlResult.wordCount > 1000) {
      fullText = htmlResult.text;
      fullTextSource = 'unpaywall_landing_page';
      fullTextWordCount = htmlResult.wordCount;
      this.logger.log(`‚úÖ Tier 2.5 SUCCESS: ${fullTextWordCount} words from publisher HTML`);
    } else {
      this.logger.log(`‚ö†Ô∏è  Tier 2.5 FAILED: ${htmlResult.error || 'Insufficient content'}`);
      // Continue to Tier 3 (PDF extraction)
    }
  }
}

// Existing Tier 3: PDF via Unpaywall (fallback)
if (!fullText && paper.doi) {
  // ... existing PDF extraction code
}
```

### Publisher Selector Enhancements

**Add New Publishers:**

1. **Elsevier/ScienceDirect** (User's example):
   - Selector: `#body`, `.Body`, `article`
   - Already implemented (line 378-379)
   - May need User-Agent header enhancement

2. **Wiley**:
   - Selector: `.article__body`, `.article-section__content`
   - ADD NEW

3. **Taylor & Francis**:
   - Selector: `.hlFld-Abstract`, `.abstractSection`
   - ADD NEW

4. **SAGE**:
   - Selector: `.body`, `.abstractSection`
   - ADD NEW

### Anti-Scraping Mitigation

**Enhanced Headers:**
```typescript
headers: {
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36...',
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en-US,en;q=0.9',
  'Referer': 'https://unpaywall.org/', // Looks like coming from Unpaywall
  'DNT': '1',
}
```

### Testing

1. Test with user's ScienceDirect example
2. Test with 10 different publishers
3. Measure success rate (target: 60-70%)
4. Test fallback to PDF when HTML fails (403 error)
5. Test word count improvement (target: 5x increase)

---

## DAY 7-8: UNIFIED EXTRACTION ORCHESTRATOR (16 hours)

### üéØ Goal

Create master orchestrator that coordinates all 5 tiers with intelligent fallbacks.

### Implementation

**File:** `backend/src/modules/literature/services/unified-fulltext-extraction.service.ts`

**Class Structure:**
```typescript
export class UnifiedFullTextExtractionService {
  constructor(
    private identifierEnrichment: IdentifierEnrichmentService,
    private sourceRouter: SourceAwareExtractionRouter,
    private grobid: GrobidExtractionService,
    private htmlService: HtmlFullTextService,
    private pdfParsingService: PDFParsingService,
    private pmcService: PMCService,
    private logger: Logger,
  ) {}

  /**
   * Main extraction method - tries all 5 tiers
   */
  async extract(paper: Paper): Promise<FullTextResult> {
    // Step 1: Enrich identifiers
    const enriched = await this.identifierEnrichment.enrichPaper(paper);

    // Step 2: Get extraction plan from router
    const plan = await this.sourceRouter.route(enriched);

    // Step 3: Execute extraction based on plan
    return await this.executePlan(plan, enriched);
  }

  private async executePlan(plan: ExtractionPlan, paper: EnrichedPaper): Promise<FullTextResult> {
    switch (plan.tier) {
      case 1:
        return await this.extractDirectContent(paper);
      case 2:
        return await this.extractFromPDF(plan.url, paper);
      case 3:
        return await this.extractViaIdentifiers(paper);
      case 4:
        return await this.advancedExtraction(plan, paper);
      case 5:
        return await this.fallbackToAbstract(paper);
    }
  }
}
```

**Tier Implementations:**

**Tier 1: Direct Content**
```typescript
private async extractDirectContent(paper: EnrichedPaper): Promise<FullTextResult> {
  // PMC papers
  if (paper.source === 'pmc' && paper.fullText) {
    return {
      success: true,
      fullText: paper.fullText,
      wordCount: paper.fullTextWordCount,
      source: 'pmc_direct',
      tier: 1,
      extractionTime: 0,
    };
  }

  // ERIC full-text
  if (paper.source === 'eric' && paper.fullText) {
    return { /* similar */ };
  }

  // Tier 1 failed, cascade to Tier 2
  return null;
}
```

**Tier 2: Direct PDFs**
```typescript
private async extractFromPDF(pdfUrl: string, paper: EnrichedPaper): Promise<FullTextResult> {
  try {
    const startTime = Date.now();
    const pdfBuffer = await this.downloadPDF(pdfUrl);
    const text = await this.pdfParsingService.extractText(pdfBuffer);
    const wordCount = this.calculateWordCount(text);

    if (wordCount > 100) {
      return {
        success: true,
        fullText: text,
        wordCount,
        source: `${paper.source}_pdf`,
        tier: 2,
        extractionTime: Date.now() - startTime,
      };
    }
  } catch (error) {
    this.logger.warn(`Tier 2 PDF extraction failed: ${error.message}`);
  }

  // Cascade to Tier 3
  return null;
}
```

**Tier 3: Identifier-Based**
```typescript
private async extractViaIdentifiers(paper: EnrichedPaper): Promise<FullTextResult> {
  // Try PMC if has PMID
  if (paper.pmid) {
    const pmcResult = await this.pmcService.fetchFullTextByPMID(paper.pmid);
    if (pmcResult) return pmcResult;
  }

  // Try Unpaywall if has DOI
  if (paper.doi) {
    const unpaywallResult = await this.extractViaUnpaywall(paper.doi);
    if (unpaywallResult) return unpaywallResult;
  }

  // Cascade to Tier 4
  return null;
}
```

**Tier 4: Advanced Extraction**
```typescript
private async advancedExtraction(plan: ExtractionPlan, paper: EnrichedPaper): Promise<FullTextResult> {
  const results = await Promise.allSettled([
    // Try GROBID if has PDF
    plan.url ? this.grobid.processPDF(await this.downloadPDF(plan.url)) : null,

    // Try publisher HTML if has landing page
    plan.landingPage ? this.htmlService.scrapeHtmlFromUrl(plan.landingPage) : null,
  ]);

  // Return first successful result
  for (const result of results) {
    if (result.status === 'fulfilled' && result.value) {
      return result.value;
    }
  }

  // Cascade to Tier 5
  return null;
}
```

**Tier 5: Fallback**
```typescript
private async fallbackToAbstract(paper: EnrichedPaper): Promise<FullTextResult> {
  if (paper.abstract && this.calculateWordCount(paper.abstract) > 50) {
    return {
      success: true,
      fullText: paper.abstract,
      wordCount: this.calculateWordCount(paper.abstract),
      source: 'abstract_only',
      tier: 5,
      extractionTime: 0,
    };
  }

  return {
    success: false,
    error: 'No full-text or abstract available',
    tier: 5,
  };
}
```

### Logging & Observability

```typescript
// Detailed tier logging
this.logger.log(`üéØ Extraction Plan for "${paper.title}":
  Source: ${paper.source}
  Identifiers: DOI=${paper.doi}, PMID=${paper.pmid}, PMC=${paper.pmcId}
  Plan: Tier ${plan.tier} (${plan.method})
  Expected Success: ${plan.expectedSuccessRate}%`);

// Tier execution logging
this.logger.log(`üîç Attempting Tier ${tier}: ${method}`);
this.logger.log(`‚úÖ Tier ${tier} SUCCESS: ${wordCount} words in ${time}ms`);
this.logger.log(`‚ö†Ô∏è Tier ${tier} FAILED: ${error} - Cascading to Tier ${tier + 1}`);
```

### Performance Tracking

```typescript
// Track tier performance metrics
this.metrics.recordExtraction({
  paperId: paper.id,
  source: paper.source,
  tierAttempts: [1, 2, 3], // Which tiers were attempted
  tierSuccess: 3, // Which tier succeeded
  totalTime: 3542, // milliseconds
  wordCount: 5234,
  method: 'unpaywall_pdf',
});
```

---

## DAY 9-10: COMPREHENSIVE TESTING (16 hours)

### üéØ Goal

Test extraction across all 8 sources with real papers.

### Test Matrix

| Source | Papers to Test | Expected Tier | Success Target |
|--------|---------------|---------------|----------------|
| Semantic Scholar | 20 papers | 2 or 3 | 80%+ |
| CrossRef | 20 papers | 3 or 5 | 60%+ |
| PubMed | 20 papers | 3 (PMC) or 5 | 70%+ |
| arXiv | 20 papers | 2 (always) | 100% |
| PMC | 20 papers | 1 (always) | 100% |
| ERIC | 20 papers | 2 or 5 | 70%+ |
| CORE | 20 papers | 2 or 3 | 80%+ |
| Springer | 20 papers | 2 (OA) or 3 | 75%+ |
| **TOTAL** | **160 papers** | **Mixed** | **80%+** |

### Test Scenarios

**Scenario 1: Ideal Case (arXiv)**
```typescript
test('arXiv paper extraction (Tier 2)', async () => {
  const paper = {
    source: 'arxiv',
    title: 'Attention Is All You Need',
    pdfUrl: 'https://arxiv.org/pdf/1706.03762.pdf',
  };

  const result = await extractor.extract(paper);

  expect(result.success).toBe(true);
  expect(result.tier).toBe(2);
  expect(result.wordCount).toBeGreaterThan(5000);
  expect(result.extractionTime).toBeLessThan(3000); // < 3s
});
```

**Scenario 2: Identifier Enrichment (PubMed)**
```typescript
test('PubMed paper ‚Üí PMC enrichment ‚Üí Tier 3', async () => {
  const paper = {
    source: 'pubmed',
    title: 'Example Paper',
    pmid: '12345678',
    // No PMC ID initially
  };

  const result = await extractor.extract(paper);

  expect(result.success).toBe(true);
  expect(result.tier).toBe(3); // Via PMC API lookup
  expect(result.wordCount).toBeGreaterThan(3000);
  expect(result.source).toBe('pmc_api');
});
```

**Scenario 3: Cascading Fallback (CrossRef)**
```typescript
test('CrossRef paper ‚Üí Unpaywall ‚Üí Publisher HTML ‚Üí Tier 3', async () => {
  const paper = {
    source: 'crossref',
    title: 'Example Paper',
    doi: '10.1234/example',
  };

  const result = await extractor.extract(paper);

  // May succeed via Unpaywall or fall to Tier 5
  if (result.success) {
    expect(result.tier).toBeGreaterThanOrEqual(3);
    expect(result.wordCount).toBeGreaterThan(1000);
  } else {
    expect(result.tier).toBe(5);
  }
});
```

**Scenario 4: GROBID vs pdf-parse Quality**
```typescript
test('GROBID extracts 5x more content than pdf-parse', async () => {
  const pdfUrl = 'https://arxiv.org/pdf/1706.03762.pdf';
  const pdfBuffer = await downloadPDF(pdfUrl);

  // Old method
  const pdfParseResult = await pdfParse(pdfBuffer);
  const pdfParseWords = calculateWordCount(pdfParseResult.text);

  // New method
  const grobidResult = await grobid.processPDF(pdfBuffer);
  const grobidWords = grobidResult.totalWords;

  expect(grobidWords).toBeGreaterThan(pdfParseWords * 3); // At least 3x better
});
```

### Integration Tests

1. **Full Workflow Test**
   - User searches ‚Üí Selects papers from different sources
   - Theme extraction triggers full-text fetch
   - Verify all sources extract successfully

2. **Performance Benchmarks**
   - Single paper extraction: < 5s average
   - Batch of 10 papers: < 20s average
   - No memory leaks after 100 extractions

3. **Error Recovery**
   - Test all tier failures cascade correctly
   - Test timeout handling (30s max per tier)
   - Test network failures (retry logic)

### Success Metrics

**Overall Targets:**
- ‚úÖ Success rate: 80%+ (vs current ~60%)
- ‚úÖ Average word count: 3000+ (vs current 500-800)
- ‚úÖ Average extraction time: < 5s
- ‚úÖ arXiv success: 100%
- ‚úÖ PMC success: 100%
- ‚úÖ Zero critical bugs

---

## DAY 11: PERFORMANCE OPTIMIZATION & CACHING (8 hours)

### üéØ Goal

Optimize extraction speed and reduce redundant API calls.

### Caching Strategy

**3-Tier Cache:**

**L1: In-Memory Cache (Fast)**
```typescript
// Cache enrichment results (PMID ‚Üí PMC ID)
const enrichmentCache = new NodeCache({
  stdTTL: 3600, // 1 hour
  maxKeys: 10000
});
```

**L2: Redis Cache (Shared)**
```typescript
// Cache extraction results
await redis.set(
  `fulltext:${paper.id}`,
  JSON.stringify(result),
  'EX', 86400 // 24 hours
);
```

**L3: Database (Persistent)**
```typescript
// Already exists in Paper model
// fullText, fullTextWordCount, fullTextStatus
```

### Parallel Processing

**Batch Extraction:**
```typescript
async extractBatch(papers: Paper[]): Promise<FullTextResult[]> {
  // Process in parallel batches of 5
  const batches = chunk(papers, 5);
  const results = [];

  for (const batch of batches) {
    const batchResults = await Promise.allSettled(
      batch.map(paper => this.extract(paper))
    );
    results.push(...batchResults);
  }

  return results;
}
```

### Request Deduplication

**Prevent duplicate API calls:**
```typescript
private activeRequests = new Map<string, Promise<FullTextResult>>();

async extract(paper: Paper): Promise<FullTextResult> {
  const key = `${paper.source}:${paper.id}`;

  // Return existing promise if already fetching
  if (this.activeRequests.has(key)) {
    return this.activeRequests.get(key);
  }

  // Create new promise
  const promise = this._extract(paper);
  this.activeRequests.set(key, promise);

  // Cleanup after completion
  promise.finally(() => this.activeRequests.delete(key));

  return promise;
}
```

### Success Metrics

- ‚úÖ Cache hit rate: > 40%
- ‚úÖ Batch extraction: 5 papers in parallel
- ‚úÖ Zero duplicate API calls
- ‚úÖ Memory usage: < 500MB for 1000 papers

---

## DAY 12: DOCUMENTATION & PRODUCTION READINESS (8 hours)

### Deliverables

1. **Architecture Documentation**
   - 5-tier extraction flow diagram
   - Source routing decision tree
   - API documentation for all new services

2. **Testing Report**
   - Results for all 160 test papers
   - Success rate per source
   - Performance benchmarks

3. **Migration Guide**
   - How to enable new extraction system
   - Feature flag configuration
   - Rollback procedure

4. **Monitoring Setup**
   - Tier success metrics dashboard
   - Extraction time tracking
   - Error rate alerts

5. **Production Runbook**
   - GROBID deployment guide
   - Cache configuration
   - Troubleshooting common issues

---

## üìä EXPECTED OUTCOMES

### Quantitative Improvements

| Metric | Before (Current) | After (Phase 10.94) | Improvement |
|--------|-----------------|---------------------|-------------|
| Success Rate | 60% | 80-85% | +33% |
| Average Words | 500-800 | 3000-5000 | 5-6x |
| Extraction Time | 5-10s | 3-5s | 40-50% faster |
| arXiv Success | 95% | 100% | +5% |
| PMC Success | 90% | 100% | +10% |
| Quality Score | 6/10 | 9/10 | +50% |

### Qualitative Improvements

‚úÖ **Intelligent Routing**: Right method for each source
‚úÖ **Better PDF Parsing**: GROBID vs pdf-parse (5-10x more content)
‚úÖ **Publisher HTML**: Unpaywall landing pages (5000+ words)
‚úÖ **Identifier Enrichment**: PMID ‚Üí PMC, DOI ‚Üí Unpaywall
‚úÖ **Source-Specific Optimization**: No wasted API calls
‚úÖ **5-Tier Fallbacks**: Always gets best available content
‚úÖ **Enterprise-Grade**: Caching, monitoring, error recovery

---

## üöÄ PRODUCTION DEPLOYMENT

### Prerequisites

1. **GROBID Service**
   - Deploy GROBID Docker container
   - Configure URL in environment variables
   - Test health endpoint

2. **Environment Variables**
   ```bash
   GROBID_URL=http://localhost:8070
   ENABLE_GROBID_EXTRACTION=true
   ENABLE_UNPAYWALL_HTML=true
   FULLTEXT_CACHE_TTL=86400
   ```

3. **Feature Flags**
   ```typescript
   USE_UNIFIED_EXTRACTION=true // New system
   FALLBACK_TO_OLD_EXTRACTION=true // Safety net
   ```

### Rollout Plan

**Week 1:** 10% of users ‚Üí New system
**Week 2:** 50% of users ‚Üí Monitor metrics
**Week 3:** 100% of users ‚Üí Full rollout

**Rollback Trigger:** Error rate > 5% OR success rate < 70%

### Monitoring Dashboards

1. **Extraction Success Dashboard**
   - Success rate per source
   - Tier distribution (how many use Tier 1, 2, 3, etc.)
   - Average word count per source

2. **Performance Dashboard**
   - Average extraction time per tier
   - Cache hit rate
   - API call volume

3. **Error Dashboard**
   - Error rate by type (timeout, 403, parsing failure)
   - Failed papers (for manual review)
   - Cascade patterns (which tiers fail most)

---

## üìö REFERENCES

### Internal Documentation

- [8_FREE_SOURCES_FULLTEXT_ANALYSIS.md](./8_FREE_SOURCES_FULLTEXT_ANALYSIS.md) - Comprehensive source analysis
- [ENHANCEMENT_PUBLISHER_HTML_EXTRACTION.md](./ENHANCEMENT_PUBLISHER_HTML_EXTRACTION.md) - Publisher HTML proposal
- [BUGFIX_GET_PAPER_CONTENT_MISSING.md](./BUGFIX_GET_PAPER_CONTENT_MISSING.md) - Related bug fix

### External References

- [GROBID Documentation](https://grobid.readthedocs.io/)
- [Unpaywall API Documentation](https://unpaywall.org/products/api)
- [PMC E-utilities Guide](https://www.ncbi.nlm.nih.gov/books/NBK25501/)
- [CrossRef API Documentation](https://www.crossref.org/documentation/retrieve-metadata/rest-api/)

### Service Files

- `backend/src/modules/literature/services/pdf-parsing.service.ts` - Current extraction
- `backend/src/modules/literature/services/html-full-text.service.ts` - HTML extraction
- `backend/src/modules/literature/services/pmc.service.ts` - PMC integration
- `backend/src/modules/literature/services/semantic-scholar.service.ts` - Semantic Scholar
- `backend/src/modules/literature/services/arxiv.service.ts` - arXiv integration
- `backend/src/modules/literature/services/core.service.ts` - CORE API
- `backend/src/modules/literature/services/springer.service.ts` - Springer integration

---

**End of Phase 10.94 Implementation Guide**

**Status:** üìã Ready for Implementation
**Prepared By:** Claude
**Date:** November 19, 2025
**Estimated Total Effort:** 96-120 hours (12 days)
**Expected ROI:** 5-6x improvement in content quality, 33% improvement in success rate
